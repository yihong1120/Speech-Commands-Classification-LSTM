{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub\n",
        "!pip install tensorflow_io"
      ],
      "metadata": {
        "id": "O50BKR28eYlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYSZUT3ZeOfo"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "# Load the Speech Commands dataset\n",
        "train_dataset, validation_dataset, test_dataset = tfds.load(\n",
        "    \"speech_commands\",\n",
        "    split=[\"train[:80%]\", \"train[80%:90%]\", \"train[90%:]\"],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True\n",
        ")\n",
        "# Define parameters\n",
        "batch_size = 32\n",
        "epochs = 10 # Shorten the training time for demonstration purposes\n",
        "num_classes = 35 # There are 35 different commands in the Speech Commands dataset\n",
        "frame_length = 16000 # Set the frame_length to 16000 because the audio sampling rate is 16 kHz\n",
        "\n",
        "# Define the data preprocessing function\n",
        "def preprocess_dataset(audio, label):\n",
        "    # Cast the data type using tf.cast\n",
        "    audio = tf.cast(audio, tf.float32)\n",
        "    # If the length of the audio file is less than the fixed frame length, pad it with zeros\n",
        "    if tf.shape(audio)[0] < frame_length:\n",
        "        padding = tf.zeros((frame_length - tf.shape(audio)[0],), dtype=tf.float32)\n",
        "        audio = tf.concat([audio, padding], axis=0)\n",
        "    # If the length of the audio file is greater than the fixed frame length, trim it to the fixed frame length\n",
        "    elif tf.shape(audio)[0] > frame_length:\n",
        "        audio = audio[:frame_length]\n",
        "    else:\n",
        "        audio = audio\n",
        "    # Use tf.numpy_function to call a NumPy function to process the tensor\n",
        "    audio = tf.numpy_function(func=np.int16, inp=[audio], Tout=tf.int16)\n",
        "    # Trim the audio file to the fixed frame length; frame_length has been set to 16000 here\n",
        "    frame_step = frame_length // 2\n",
        "    audio = tf.signal.frame(audio, frame_length, frame_step, pad_end=True)\n",
        "    # Convert the label to a one-hot vector\n",
        "    label = tf.one_hot(label, depth=num_classes)\n",
        "    return audio, label\n",
        "\n",
        "# Map and preprocess the dataset\n",
        "train_data = train_dataset.map(preprocess_dataset)\n",
        "train_data = train_data.shuffle(buffer_size=1024).batch(batch_size).repeat().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "validation_data = validation_dataset.map(preprocess_dataset)\n",
        "validation_data = validation_data.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_data = test_dataset.map(preprocess_dataset)\n",
        "test_data = test_data.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Define the model, optimizer, and loss function\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.LSTM(64, return_sequences=True, input_shape=(None, frame_length)),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.LSTM(64),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "train_size = tf.data.experimental.cardinality(train_dataset).numpy()\n",
        "steps_per_epoch = int(train_size / batch_size)\n",
        "model.fit(train_data, epochs=epochs, validation_data=validation_data, steps_per_epoch=steps_per_epoch)\n",
        "\n",
        "# Evaluate the model\n",
        "model.evaluate(test_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBOup8vLuusx",
        "outputId": "3ec0b1ac-8d36-4b20-c2fc-94e6150444a4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2137/2137 [==============================] - 72s 31ms/step - loss: 1.6402 - accuracy: 0.6205 - val_loss: 1.5353 - val_accuracy: 0.6299\n",
            "Epoch 2/10\n",
            "2137/2137 [==============================] - 67s 31ms/step - loss: 1.5369 - accuracy: 0.6326 - val_loss: 1.5373 - val_accuracy: 0.6299\n",
            "Epoch 3/10\n",
            "2137/2137 [==============================] - 64s 30ms/step - loss: 1.5328 - accuracy: 0.6325 - val_loss: 1.5330 - val_accuracy: 0.6299\n",
            "Epoch 4/10\n",
            "2137/2137 [==============================] - 69s 32ms/step - loss: 1.5319 - accuracy: 0.6325 - val_loss: 1.5352 - val_accuracy: 0.6299\n",
            "Epoch 5/10\n",
            "2137/2137 [==============================] - 64s 30ms/step - loss: 1.5294 - accuracy: 0.6326 - val_loss: 1.5319 - val_accuracy: 0.6299\n",
            "Epoch 6/10\n",
            "2137/2137 [==============================] - 67s 31ms/step - loss: 1.5284 - accuracy: 0.6326 - val_loss: 1.5323 - val_accuracy: 0.6299\n",
            "Epoch 7/10\n",
            "2137/2137 [==============================] - 78s 36ms/step - loss: 1.5268 - accuracy: 0.6326 - val_loss: 1.5315 - val_accuracy: 0.6299\n",
            "Epoch 8/10\n",
            "2137/2137 [==============================] - 65s 31ms/step - loss: 1.5259 - accuracy: 0.6326 - val_loss: 1.5317 - val_accuracy: 0.6299\n",
            "Epoch 9/10\n",
            "2137/2137 [==============================] - 63s 30ms/step - loss: 1.5251 - accuracy: 0.6325 - val_loss: 1.5317 - val_accuracy: 0.6299\n",
            "Epoch 10/10\n",
            "2137/2137 [==============================] - 74s 35ms/step - loss: 1.5243 - accuracy: 0.6325 - val_loss: 1.5306 - val_accuracy: 0.6299\n",
            "268/268 [==============================] - 8s 28ms/step - loss: 1.5215 - accuracy: 0.6334\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.5215253829956055, 0.6333762407302856]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "\n",
        "# Load the audio file to be predicted\n",
        "filename = \"path/to/audio/file.wav\"\n",
        "audio, sr = librosa.load(filename, sr=16000)\n",
        "\n",
        "# Create the model input\n",
        "audio = tf.expand_dims(audio, axis=0)\n",
        "audio = tf.expand_dims(audio, axis=-1)\n",
        "\n",
        "# Make predictions\n",
        "prediction = model.predict(audio)\n",
        "predicted_label = np.argmax(prediction)\n"
      ],
      "metadata": {
        "id": "dCKUITK6iML8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}